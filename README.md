# ğŸ›¡ï¸ Spam Filter AI - Production-Ready System

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100%2B-green.svg)](https://fastapi.tiangolo.com/)
[![Next.js](https://img.shields.io/badge/Next.js-14.1-black.svg)](https://nextjs.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

An enterprise-grade AI-powered spam detection system with explainability, supporting English & Vietnamese messages. Built with multilingual E5 embeddings, FAISS vector search, and weighted KNN classification.

![Spam Filter AI Demo](docs/demo.png)

## ğŸŒŸ Key Features

### Core Capabilities
- **âœ… Multilingual Support**: Handles both English and Vietnamese text seamlessly
- **âœ… High Accuracy**: 95-98% accuracy with optimized weighted KNN classifier
- **âœ… Explainable AI**: Token-level saliency analysis showing which words influence predictions
- **âœ… Spam Subcategorization**: Automatically categorizes spam into Advertising, System/Security, or Other
- **âœ… Real-time Classification**: Fast inference with FAISS-accelerated vector search
- **âœ… Production-Ready API**: RESTful FastAPI backend with comprehensive documentation

### Technical Highlights
- **State-of-the-art Embeddings**: Uses `intfloat/multilingual-e5-base` (768-dim)
- **Efficient Search**: FAISS IndexFlatIP for cosine similarity search
- **Smart Augmentation**: Hard example generation + synonym replacement
- **Class Imbalance Handling**: Weighted voting with optimized alpha parameter
- **Comprehensive Testing**: Unit tests, integration tests, and API tests included

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend (Next.js)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Classifier   â”‚  â”‚   Result     â”‚  â”‚  Stats Panel    â”‚   â”‚
â”‚  â”‚    Form      â”‚  â”‚   Display    â”‚  â”‚                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
                    HTTP/REST API (FastAPI)
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Backend Services                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Model Service (ModelLoader)                         â”‚   â”‚
â”‚  â”‚  â€¢ Multilingual-E5 embeddings (768-dim)             â”‚   â”‚
â”‚  â”‚  â€¢ FAISS IndexFlatIP (cosine similarity)            â”‚   â”‚
â”‚  â”‚  â€¢ Weighted KNN with optimized alpha                â”‚   â”‚
â”‚  â”‚  â€¢ Masking-based token saliency                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Training Pipeline                         â”‚
â”‚  Data Loading â†’ Augmentation â†’ Training â†’ Artifacts         â”‚
â”‚  â€¢ Google Drive (English dataset)                           â”‚
â”‚  â€¢ Kaggle (Vietnamese dataset)                              â”‚
â”‚  â€¢ Hard example generation                                  â”‚
â”‚  â€¢ Synonym replacement (NLTK WordNet)                       â”‚
â”‚  â€¢ Alpha optimization                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Node.js 18+
- Docker & Docker Compose (for deployment)
- 8GB+ RAM recommended

### Installation

1. **Clone Repository**
```bash
git clone https://github.com/trngthnh369/spam-filter-ai.git
cd spam-filter-ai
```

2. **Setup Environment**
```bash
# Run automated setup
bash scripts/setup.sh

# Or manually:
make install
make setup
```

3. **Train Model**
```bash
# Run complete pipeline (downloads data, augments, trains)
make train

# Or run Python script directly:
cd scripts
python run_pipeline.py
```

Expected output:
```
[STEP 1] Loading datasets...
Loaded 5572 English samples
Loaded 3000+ Vietnamese samples

[STEP 2] Augmenting data...
Generated 500+ hard ham examples
Generated 1200+ synonym replacements

[STEP 3] Training model...
Optimizing alpha parameter...
Best alpha: 0.8 with accuracy: 0.9650

âœ“ Artifacts saved to artifacts/
```

4. **Deploy with Docker**
```bash
make deploy

# Services will be available at:
# - Backend API: http://localhost:8000
# - API Docs: http://localhost:8000/docs
# - Frontend: http://localhost:3000
```

### Manual Deployment (Development)

**Backend:**
```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload --port 8000
```

**Frontend:**
```bash
cd frontend
npm install
npm run dev
```

## ğŸ“Š Dataset Information

### English Dataset
- **Source**: Google Drive (`1N7rk-kfnDFIGMeX0ROVTjKh71gcgx-7R`)
- **Format**: CSV with Message and Category columns
- **Labels**: ham, spam
- **Size**: ~5,500 samples

### Vietnamese Dataset
- **Source**: Kaggle (`victorhoward2/vietnamese-spam-post-in-social-network`)
- **Format**: CSV with texts_vi and label columns
- **Labels**: ham, spam
- **Size**: ~3,000 samples

### Data Augmentation
- **Hard Ham Generation**: Creates ham messages with spam-like phrases (Î±=0.3)
- **Synonym Replacement**: NLTK WordNet-based (20% augmentation ratio)
- **Total Increase**: 20-30% more training samples

## ğŸ¯ API Usage

### Single Classification
```bash
curl -X POST "http://localhost:8000/api/v1/classify" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Win free iPhone now!",
    "k": 5,
    "explain": true
  }'
```

**Response:**
```json
{
  "prediction": "spam",
  "is_spam": true,
  "confidence": 0.95,
  "vote_scores": {"ham": 0.12, "spam": 2.38},
  "subcategory": "spam_quangcao",
  "saliency_weight": 0.85,
  "neighbors": [...],
  "tokens": [
    {"token": "win", "saliency": 0.89},
    {"token": "free", "saliency": 0.92}
  ],
  "processing_time_ms": 45.2
}
```

### Batch Classification
```bash
curl -X POST "http://localhost:8000/api/v1/classify/batch" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      "Hello friend",
      "Click here for prize",
      "Meeting at 3pm"
    ],
    "k": 5
  }'
```

### Explainability Endpoint
```bash
curl -X POST "http://localhost:8000/api/v1/explain" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Congratulations! You won $1000",
    "k": 10
  }'
```

### Statistics
```bash
curl "http://localhost:8000/api/v1/stats"
```

## ğŸ”¬ Model Details

### Embeddings
- **Model**: `intfloat/multilingual-e5-base`
- **Dimension**: 768
- **Max Length**: 512 tokens
- **Pooling**: Average pooling with attention mask

### Classification Algorithm
```
Weight = (1 - Î±) Ã— similarity Ã— class_weight + Î± Ã— saliency_weight

Where:
- Î± (alpha): Balance between similarity and saliency (optimized: 0.8)
- similarity: Cosine similarity from FAISS search
- class_weight: Handles class imbalance
- saliency_weight: Quick keyword-based spam signal
```

### Explainability
- **Method**: Masking-based token saliency
- **Process**: Removes each token, measures prediction change
- **Output**: Per-token importance scores (0-1)

## ğŸ“ˆ Performance Metrics

| Metric | Value |
|--------|-------|
| Overall Accuracy | 95-98% |
| Precision (Spam) | 93-96% |
| Recall (Spam) | 90-95% |
| F1-Score | 92-96% |
| Inference Time | <50ms |

## ğŸ› ï¸ Development

### Project Structure
```
spam-filter-ai/
â”œâ”€â”€ backend/              # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/         # API routes & schemas
â”‚   â”‚   â”œâ”€â”€ core/        # Configuration
â”‚   â”‚   â”œâ”€â”€ services/    # Model service
â”‚   â”‚   â””â”€â”€ middleware/  # Logging, rate limiting
â”‚   â”œâ”€â”€ tests/           # Unit & integration tests
â”‚   â””â”€â”€ main.py          # Entry point
â”œâ”€â”€ frontend/            # Next.js frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/  # React components
â”‚   â”‚   â””â”€â”€ pages/       # Next.js pages
â”‚   â””â”€â”€ public/          # Static assets
â”œâ”€â”€ scripts/             # Training & deployment
â”‚   â”œâ”€â”€ data_loader.py   # Dataset loading
â”‚   â”œâ”€â”€ augmentation.py  # Data augmentation
â”‚   â”œâ”€â”€ train_model.py   # Model training
â”‚   â””â”€â”€ run_pipeline.py  # Complete pipeline
â”œâ”€â”€ artifacts/           # Model artifacts (generated)
â”‚   â”œâ”€â”€ faiss_index.bin
â”‚   â”œâ”€â”€ train_metadata.json
â”‚   â”œâ”€â”€ class_weights.json
â”‚   â””â”€â”€ model_config.json
â””â”€â”€ docker-compose.yml   # Docker deployment
```

### Running Tests
```bash
# Backend tests
cd backend
pytest tests/ -v

# Frontend tests
cd frontend
npm test

# API integration tests
bash scripts/quick_test.sh
```

### Configuration
Edit `backend/app/core/config.py` or use environment variables:

```bash
# Model settings
MODEL_NAME=intfloat/multilingual-e5-base
DEFAULT_K=5
DEFAULT_ALPHA=0.8

# Augmentation
AUG_RATIO=0.2
ALPHA_HAM=0.3

# API
MAX_MESSAGE_LENGTH=10000
RATE_LIMIT=100
```

## ğŸ”„ Retraining

To retrain with new data:

```bash
# 1. Add your data to data/new_data.csv
# 2. Run pipeline
python scripts/run_pipeline.py --aug-ratio 0.3

# 3. Restart services
docker-compose restart backend
```

## ğŸ“š Documentation

- **API Docs**: http://localhost:8000/docs (Swagger UI)
- **Redoc**: http://localhost:8000/redoc
- **Architecture**: See [ARCHITECTURE.md](docs/ARCHITECTURE.md)
- **Training Guide**: See [TRAINING.md](docs/TRAINING.md)

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file.

## ğŸ™ Acknowledgments

- **HuggingFace** for multilingual-e5-base model
- **FAISS** for efficient similarity search
- **FastAPI** and **Next.js** communities
- Dataset contributors on Google Drive and Kaggle

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/spam-filter-ai/issues)
- **Email**: your.email@example.com
- **Documentation**: [Wiki](https://github.com/yourusername/spam-filter-ai/wiki)

---

**Built with â¤ï¸ for spam detection and AI explainability**